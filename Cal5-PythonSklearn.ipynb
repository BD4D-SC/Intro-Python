{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 250px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 200px; display: inline\" alt=\"Python\"/></a> pour Statistique et Science des Données\n",
    "\n",
    "L'objectif de ces tuteuriels sous la forme de calepins (*notebooks*) est d'introduire l'utilisation de Python pour tout d'abord la préparation (*data munging* ou *wrangling*) de données, éventuellement massives, puis leur valorisation en enchaînant phases d'exploration puis de modélisaiton ou d'apprentissage. Des exemples plus détaillés sur des données spécifiques (en R et en python) sont également proposés sur [wikistat.fr](\\http://wikistat.fr).\n",
    "\n",
    "Les documents ci-dessous sont des calepins (*notebooks*) à télécharger et ouvrir dans *Jupyter*.\n",
    "\n",
    "* [Introduction à Python pour  Calcul Scientifique - Statistique](http://wikistat.fr/Notebooks/Cal1-introPython.ipynb)\n",
    "* [Trafic de Données avec Python `Pandas`](http://wikistat.fr/Notebooks/Cal2-PythonPanda.ipynb)\n",
    "* [Graphiques en Python et autre tutoriels](http://wikistat.fr/Notebooks/Cal3-PythonGraphes.ipynb)\n",
    "* [Programmation élémentaire en Python](http://wikistat.fr/Notebooks/Cal4-PythonProgram.ipynb)\n",
    "* [Apprentissage Statistique et/ou Machine avec Python `Scikit-Learn`](http://wikistat.fr/Notebooks/Cal5-PythonSklearn.ipynb)\n",
    "* [Apprentissage de Spark avec PySpark](http://wikistat.fr/Notebooks/Cal6-PythonSpark.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage Statistique / Machine avec <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 150px; display: inline\" alt=\"Python\"/></a> & <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 180px; display: inline\" alt=\"Scikit-Learn\"/></a>\n",
    "**Résumé**: Après la [préparation](http://wikistat.fr/Notebooks/Cal2-PythonPanda.ipynb) des données, ce calepin introduit l'utilisation de la librairie `scikit-learn` pour la modélisation et l'apprentissage. Pourquoi utiliser ` scikit-learn` ? Ou non ? Liste des fonctionnalités, quelques exemples de mise en oeuvre: exploration ([ACP](http://wikistat.fr/pdf/st-m-explo-acp.pdf)}, [AFCM](http://wikistat.fr/pdf/st-m-explo-afcm.pdf), [$k$-means)(http://wikistat.fr/pdf/st-m-explo-classif.pdf)), modélisation ([régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf), [$k$-plus proches voisins](http://wikistat.fr/pdf/st-m-app-add.pdf), [arbres de décision](http://wikistat.fr/pdf/st-m-app-cart.pdf), [forêts aléatoires](http://wikistat.fr/pdf/st-m-app-agreg.pdf). Optimisation des paramètres (complexité) des modèles par [validation croisée](http://wikistat.fr/pdf/st-m-app-risque-estim.pdf). Des fonctionalités plus avancées de `Scikit-learn` sont abordés dans d'autres calepins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "### 1.1 `Scikit-learn` *vs.*  R\n",
    "L'objectif de ce tutoriel est d'introduire l'utilisation de la librairie `scikit-learn` de Python. Seuls l'utilisation directe des fonctions d'exploration et de modélisation sont abordées d'une manière analogue à la mise en oeuvre de R dont les librairies offrent l'accès à bien plus d eméthodes. Se pose alors la question: quand utiliser `scikit-learn` de Python plutôt que par exemple `caret` de R plus complet et plus simple d'emploi?\n",
    "\n",
    "Le choix repose sur les points suivants:\n",
    "- **Attention** cette librairie manipule des objets de classe `array` de `numpy` *chargés en mémoire* et donc de taille limitée par la RAM de l'ordinateur; de façon analogue R charge en RAM des objets de type `data.frame`.\n",
    "- **Attention** toujours, `scikit-learn` (0.17) ne reconnaît pas (ou pas encore ?) la classe `DataFrame` de `pandas`; `scikit-learn` utilise la classe `array` de `numpy`. C'est un problème pour la gestion de variables qualitatives complexes. Une variable binaire est simplement remplacée par un codage $(0,1)$ mais, en présence de plusieurs modalités, traiter celles-ci comme des entiers n'a pas de sens statistique et remplacer une variable qualitative par l'ensemble des indicatrices (*dummy variables* $(0,1)$) de ses modalités  complique les stratégies de sélection de modèle et rend inexploitable l'interprétation statistique. \n",
    "- Les implémentations en Python de certains algorithmes dans `scikit-learn` sont aussi efficaces (*e.g.* $k$-means), voire beaucoup plus efficaces (*e.g.* $k$-nn) pour des données volumineuses.\n",
    "- R offre beaucoup plus de possibilités pour une exploration, des recherches et comparaisons de modèles, des interprétations mais les capacités de parallélisation de Python sont nettement plus performantes. Plus précisément, l'introduction de nouvelles librairies n'est pas ou peu contraintes dans R, alors que celle de nouvelles méthodes dans `scikit-learn` se fait sous contrôle d'un groupe qui en contrôle la pertinence et l'efficacité. \n",
    "\n",
    "En conséquences:\n",
    "- Préférer R et ses libraires si la présentation des résultats et surtout leur interprétation (modèles) est prioritaire, si  l'utilisation et / ou la comparaison de beaucoup de méthodes est recherchée.\n",
    "- Préférer Python et `scikit-learn` pour mettre au point une chaîne de traitements (*pipe line*) opérationnelle de l'extraction à une analyse privilégiant la prévision brute à l'interprétation et pour des données quantitatives ou rendues quantitatives (\"vectorisation\" de corpus de textes).\n",
    "\n",
    "En revanche, si les données sont trop volumineuses pour la taille du disque et distribuées sur les n\\oe uds d'un * cluster* avec *Hadoop*, consulter le [calepin](http://wikistat.fr/Notebooks/Cal6-PythonSpark.ipynb) suivant pour l'utilisation de *Spark*.\n",
    "\n",
    "\n",
    "## 1.2 Fonctions de `scikit-learn`\n",
    "La communauté qui développe cette librairie est très active, elle évolue rapidement.  Ne pas hésiter à consulter la [documentation](http://scikit-learn.org/stable/user_guide.html) pour des compléments. Voici une sélection de ses principales fonctionnalités.\n",
    "- Transformations (standardisation, discrétisation binaire, regroupement de modalités, imputations rudimentaires de données manquantes) , \"vectorisation\" de corpus de textes (encodage, catalogue, Tf-idf), images.\n",
    "- Exploration: ACP, classification non supervisée (mélanges gaussiens, propagation d'affinité, ascendante hiérarchique, SOM,...) \n",
    "- Modéle linéaire général avec pénalisation (ridge, lasso, elastic net...), analyse discriminante linéaire et quadratique,  $k$ plus proches voisins,  processus gaussiens, classifieur bayésien naïf, arbres de régression et classification (CART), agrégation de modèles (bagging, random forest, adaboost, gradient tree boosting), SVM (classification, régression, détection d'atypiques...).\n",
    "- Algorithmes de validation croisée (loo, k-fold, VC stratifiée...) et sélection de modèles, optimisation sur une grille de paramètres, séparation aléatoire apprentissage et test, enchaînement (pipe line) de traitements, courbe ROC. \n",
    "\n",
    "En résumé, cette librairie est focalisée sur les aspects \"machine\" de l'apprentissage de données quantitatives (séries, signaux, images) volumineuses tandis que R intègre l'analyse de variables qualitatives complexes et l'interprétation statistique fine des résultats au détriment parfois de l'efficacité des calculs.\n",
    "\n",
    "### 1.3 Objectif\n",
    "L'objectif est d'illustrer la mise en oeuvre de quelques fonctionnalités. Consulter la [documentation](http://scikit-learn.org/stable/user_guide.html) et ses nombreux [exemples](http://scikit-learn.org/stable/auto_examples/index.html) pour plus de détails sur l'utilisation de `scikit-learn`. \n",
    "\n",
    "Deux jeux de données élémentaires sont utilisés. Celui déjà étudié avec `pandas` et concernant le naufrage du [Titanic](http://wikistat.fr/Notebooks/Cal2-PythonPanda.html) mélange des variables explicatives qualitatives et quantitatives dans un objet de la classe `DataFrame`. Pour être utilisé dans `scikit-learn` les données doivent être transformées en un objet de classe `Array` de `numpy` en remplaçant les variables qualitatives par les indicatrices de leurs modalités.  L'autre ensemble de données est entièrement quantitatif. C'est un problème classique et simplifié de [reconnaissance de caractères](http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits) qui est inclus dans la librairie `scikit-learn`.\n",
    "\n",
    "Ces données sont explorées par [analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf) (caractères) ou [analyse multiple des correspondances](http://wikistat.fr/pdf/st-m-explo-afcm.pdf) (titanic), [classifiées](http://wikistat.fr/pdf/st-m-explo-classif.pdf) puis modélisées par [régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf) (titanic), [$k$- plus proches voisins](http://wikistat.fr/pdf/st-m-app-add.pdf) (caractères), [arbres de discrimination](http://wikistat.fr/pdf/st-m-app-cart.pdf), et [forêts aléatoires](http://wikistat.fr/pdf/st-m-app-agreg.pdf). Les paramètres de complexité des modèles  sont optimisés par minimisation de l'[erreur de prévision](http://wikistat.fr/pdf/st-m-app-risque-estim.pdf) estimée par [validation croisée](http://wikistat.fr/pdf/st-m-app-risque-estim.pdf).  \n",
    "\n",
    "D'autres fonctionnalités sont laissées momentanément de côté; cela concerne les possibilités d'enchaînement (*pipeline*) de méthodes et d'automatisation. Il s'agit, par exemple, de répéter automatiquement la séparation aléatoire des échantillons apprentissage et test pour estimer les distributions des erreurs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploration multidimensionnelle\n",
    "### 2.1 Les données\n",
    "#### Les données \"Caractères\"\n",
    "Il s'agit d'explorer celles de reconnaissance de caractères dont les procédés d'obtention et prétraitements sont décrits sur le [site de l'UCI](http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits) (Lichman, 2013). Les chiffres ont été saisies sur des tablettes à l'intérieur de cadres de résolution $500\\times 500$. Des procédures de normalisation,  ré-échantillonnage spatial puis de lissage ont été appliquées. Chaque caractère apparaît finalement discrétisé sous la forme d'une matrice $8\\times 8$ de pixels à 16 niveaux de gris et identifié par un label. Les données sont archivées sous la forme d'une matrice ou tableau à trois indices. Elles sont également archivées après vectorisation des images sous la forme d'une matrice à $p=64$ colonnes.\n",
    "\n",
    "L'étude du même type de données mais nettement plus complexes (MNIST): 60 000 caractères représentés par des images de 784 pixels (26 $\\times$ 26) fait l'objet d'un autre calepin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importations \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "# les données\n",
    "digits = datasets.load_digits()\n",
    "# Contenu et mode d'obtention\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sous forme d'un cube d'images 1797 x 8x8\n",
    "print(digits.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sous forme d'une matrice 1797 x 64\n",
    "print(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Label réel de chaque caractère\n",
    "print(digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un aperçu des images à discriminer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_and_labels = list(zip(digits.images, \n",
    "   digits.target))\n",
    "for index, (image, label) in  enumerate(images_and_labels[:8]):\n",
    "     plt.subplot(2, 4, index + 1)\n",
    "     plt.axis('off')\n",
    "     plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "     plt.title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Données Titanic\n",
    "Les données sur le naufrage du Titanic sont décrites dans le [calepin](http://wikistat.fr/Notebooks/Cal2-PythonPanda.html) consacré à `pandas`. Reconstruire la table de données en lisant le fichier `.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lire les données d'apprentissage\n",
    "import pandas as pd\n",
    "# path=''  # si les données sont déjà dans le répertoire courant\n",
    "path='http://www.math.univ-toulouse.fr/~besse/Wikistat/data/'\n",
    "df=pd.read_csv(path+'titanic-train.csv',skiprows=1,header=None,usecols=[1,2,4,5,9,11],\n",
    "  names=[\"Surv\",\"Classe\",\"Genre\",\"Age\",\"Prix\",\"Port\"],dtype={\"Surv\":object,\"Classe\":object,\"Genre\":object,\"Port\":object})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape # dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Redéfinir les types \n",
    "df[\"Surv\"]=pd.Categorical(df[\"Surv\"],ordered=False)\n",
    "df[\"Classe\"]=pd.Categorical(df[\"Classe\"],ordered=False)\n",
    "df[\"Genre\"]=pd.Categorical(df[\"Genre\"],ordered=False)\n",
    "df[\"Port\"]=pd.Categorical(df[\"Port\"],ordered=False)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier que les données contiennent des valeurs manquantes, faire des imputations à la médiane d'une valeur quantitative manquante ou la modalité la plus fréquente d'une valeur qualitative absente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imputation des valeurs manquantes\n",
    "df[\"Age\"]=df[\"Age\"].fillna(df[\"Age\"].median())\n",
    "df.Port=df[\"Port\"].fillna(\"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuer en transformant les variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Discrétiser les variables quantitatives\n",
    "df[\"AgeQ\"]=pd.qcut(df.Age,3,labels=[\"Ag1\",\"Ag2\",\"Ag3\"])\n",
    "df[\"PrixQ\"]=pd.qcut(df.Prix,3,labels=[\"Pr1\",\"Pr2\",\"Pr3\"])\n",
    "# redéfinir les noms des modalités \n",
    "df[\"Surv\"]=df[\"Surv\"].cat.rename_categories([\"Vnon\",\"Voui\"])\n",
    "df[\"Classe\"]=df[\"Classe\"].cat.rename_categories([\"Cl1\",\"Cl2\",\"Cl3\"])\n",
    "df[\"Genre\"]=df[\"Genre\"].cat.rename_categories([\"Gfem\",\"Gmas\"])\n",
    "df[\"Port\"]=df[\"Port\"].cat.rename_categories([\"Pc\",\"Pq\",\"Ps\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analyse en composantes principales\n",
    "La fonction d'[analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf) (ACP) est surtout adaptée à l'analyse de signaux, de nombreuses options ne sont pas disponibles notamment les graphiques usuels (biplot, cercle des corrélations...). En revanche des résultats sont liés à la version probabiliste de l'ACP sous hypothèse d'une distribution gaussienne multidimensionnelle des données. **Attention**, l'ACP est évidemment centrée mais par réduite. L'option n'est pas prévue et les variables doivent être réduites (fonction `sklearn.preprocessing.scale`) avant si c'est nécessaire. L'attribut `transform` désigne les composantes principales, sous-entendu: transformation par réduction de la dimension; `n_components` fixe le nombre de composantes retenues, par défaut toutes; l'attribut `components_` contient les `n_components` vecteurs propres mais non normalisés, c'est-à-dire de norme carrée la valeur propre associée, et donc à utiliser pour représenter les variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X=digits.data\n",
    "y=digits.target\n",
    "target_name=[0,1,2,3,4,5,6,7,8,9]\n",
    "# définition de la commande\n",
    "pca = PCA()\n",
    "# Estimation, calcul des composantes principales\n",
    "C = pca.fit(X).transform(X)\n",
    "# Décroissance de la variance expliquée\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagramme boîte des premières composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot(C[:,0:20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Représentation des caractères dans le premier plan principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(C[:,0], C[:,1], c=y, label=target_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le même graphique avec une légende mais moins de couleurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attention aux indentations\n",
    "plt.figure()\n",
    "for c, i, target_name in zip(\"rgbcmykrgb\",[0,1,2,3,4,5,6,7,8,9], target_name):\n",
    "       plt.scatter(C[y == i,0], C[y == i,1], c=c, label=target_name)\n",
    "plt.legend()\n",
    "plt.title(\"ACP Digits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphique en trois dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "ax.scatter(C[:, 0], C[:, 1], C[:, 2], c=y, cmap=plt.cm.Paired)\n",
    "ax.set_title(\"ACP: trois premieres composantes\")\n",
    "ax.set_xlabel(\"Comp1\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"Comp2\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"Comp3\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'autres versions d'analyse en composantes principales sont proposées : {\\it kernel PCA, sparse PCA}..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Classification non supervisée\n",
    "Exécution de l'algorithme de [classification non supervisée](http://wikistat.fr/pdf/st-m-explo-classif.pdf) (*clustering*) $k$-means dans un cas simple: le nombre des classes : paramètre `n_clusters` (8 par défaut) est *a priori* connu. D'autres paramètres peuvent être précisés ou laissés à leur valeur par défaut. Le nombre `max_iter` d'itérations (300), le nombre `n_init` (10) d'exécutions parmi lesquelles la meilleure en terme de minimisation de la variance intra-classe est retenue, le mode `init` d'initialisation qui peut être `k-means++` (par défaut) pour en principe accélérer la convergence, `random` parmi les données, ou une matrice déterminant les centres initiaux. \n",
    "\n",
    "L'option `n_jobs` permet d'exécuter les `n_init` en parallèle. Pour la valeur -1, tous les processeurs sauf un sont utilisés.\n",
    "\n",
    "Les attributs en sortie contiennent les centres: `cluster_centers_`, les numéros de classe de chaque observation: `labels_`. \n",
    "\n",
    "De nombreux critères à consulter dans la documentation sont proposés pour évaluer la qualité d'une classification non-supervisée.\n",
    "\n",
    "On se contente des options par défaut dans cette illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "est=KMeans(n_clusters=10)\n",
    "est.fit(X)\n",
    "classe=est.labels_\n",
    "print(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table=pd.crosstab(classe,y)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les vraies classes étant connues, il est facile de construire une matrice de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(table)\n",
    "plt.title(\"Matrice de Confusion\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'est pas plus difficile de représenter les classes dans les coordonnées de l'ACP. C'est la variable `classe` qui définit les couleurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "ax.scatter(C[:, 0], C[:, 1], C[:, 2], c=classe,cmap=plt.cm.Paired)\n",
    "ax.set_title(\"ACP: trois premieres composantes\")\n",
    "ax.set_xlabel(\"Comp1\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"Comp2\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"Comp3\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analyse multiple des correspondances\n",
    "Les données \"Titanic\" regroupent des variables qualitatives et quantitatives. Après recodage en classes (discrétisation) des variables quantitatives, la table obtenue se prête à une [analyse](http://wikistat.fr/pdf/st-m-explo-afcm.pdf) factorielle multiple des correspondances}. Cette méthode n'est pas présente dans les librairies courantes de python mais disponible sous la forme d'une fonction [`mca.py`](https://pypi.python.org/pypi/mca/1.0). Il est possible d'installer la librairie correspondante, par la commande `pip` ou `conda` selon l'installation, ou de simplement charger le seul module [`mca.py`](http://wikistat/programmes/mca.py) dans le répertoire courant. \n",
    "\n",
    "*Remarque*, il ne serait pas très compliqué de recalculer directement les composantes de l'AFCM à partir de la SVD du tableau disjonctif complet est obtenu en remplaçant chaque variables par les indicatrices (*dummy variables*) de ses modalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Suppression des variables quantitatives\n",
    "# pour l'AFCM\n",
    "df_q=df.drop([\"Age\",\"Prix\"],axis=1)\n",
    "df_q.head()\n",
    "# Indicatrices\n",
    "dc=pd.DataFrame(pd.get_dummies(df_q[[\"Surv\",\"Classe\",\"Genre\",\"Port\",\"AgeQ\",\"PrixQ\"]]))\n",
    "dc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de l'AFCM et représentations graphiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mca import mca\n",
    "mca_df=mca(dc,benzecri=False)\n",
    "# Valeurs singulières\n",
    "print(mca_df.L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Composantes principales des colonnes (modalités)\n",
    "print(mca_df.fs_c())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Premier plan principal\n",
    "col=[1,1,2,2,2,3,3,5,5,5,6,6,6,7,7,7]\n",
    "plt.scatter(mca_df.fs_c()[:, 0],mca_df.fs_c()[:, 1],c=col)\n",
    "for i, j, nom in zip(mca_df.fs_c()[:, 0],mca_df.fs_c()[:, 1], dc.columns):\n",
    "       plt.text(i, j, nom)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour l'ACP et au contraire de R (cf. [`FactoMineR`](http://factominer.free.fr/)), les librairies Python sont pauvres en fonctions graphiques directement adaptées à l'AFCM. Le graphique est construit à partir des fonctions de `MatPlotLib`. Remarquer l'évidente redondance entre la variable `Prix` et celle `Classe`. Il serait opportun d'en déclarer une supplémentaire. \n",
    "\n",
    "Il est alors facile de construire des classifications non supervisées des modalités des variables ou des passagers à partir de leurs composantes respectives quantitatives, ce qui revient à considérer des distances dites du $\\chi^2$ entre ces objets. Très utilisées en marketing (segmentation de clientèle), cette stratégie n'a pas grand intérêt sur ces données. Elle est développée dans un autre [calepin](http://www.math.univ-toulouse.fr/~besse/Wikistat/Notebooks/turor-visa_premier.html) sur des données bancaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Modélisation et apprentissage statistiques\n",
    "Voici des exemples de mise en oeuvre de quelques unes des méthodes de modélisation / apprentissage statistique les plus utilisées.\n",
    "### 3.1 Extraction des échantillons\n",
    "Le travail préliminaire consiste à séparer les échantillons en une partie *apprentissage* et une autre de *test* pour estimer sans biais l'[erreur de prévision](http://wikistat.fr/pdf/st-m-app-risque-estim.pdf). L'optimisation (biais-variance) de la complexité des modèles est réalisée en minimisant l'erreur estimée par [validation croisée](http://wikistat.fr/pdf/st-m-app-risque-estim.pdf). \n",
    "\n",
    "#### Caractères "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titanic\n",
    "Il est nécessaire de transformer les données car `scikit-learn` ne reconnaît pas la classe `DataFrame` de `pandas` ce qui est bien dommage. Les variables qualitatives sont comme précédemment remplacées par les indicatrices de leurs modalités et les variables quantitatives conservées. Cela introduit une évidente redondance dans les données mais les procédures de sélection de modèle feront le tri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Table de départ\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Table des indicatrices\n",
    "df1=pd.get_dummies(df_q[[\"Surv\",\"Classe\",\"Genre\",\"Port\",\"AgeQ\",\"PrixQ\"]])\n",
    "# Une seule indicatrice par variable binaire\n",
    "df1=df1.drop([\"Surv_Vnon\",\"Genre_Gmas\"],axis=1)\n",
    "# Variables quantitatives\n",
    "df2=df[[\"Age\",\"Prix\"]]\n",
    "# Concaténation\n",
    "df_c=pd.concat([df1,df2],axis=1)\n",
    "# Vérification\n",
    "df_c.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction des échantillons apprentissage et test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables explicatives\n",
    "T=df_c.drop([\"Surv_Voui\"],axis=1)\n",
    "# Variable à modéliser\n",
    "z=df_c[\"Surv_Voui\"]\n",
    "# Extractions\n",
    "T_train,T_test,z_train,z_test=train_test_split(T,z,test_size=0.2,random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention**: l'échantillon test des données \"Titanic\" est relativement petit, l'estimation de l'erreur de prévision est donc sujette à caution car probablement de grande variance. Il suffit de changer l'initialisation (paramètre ` random_state`) et ré-exécuter les scripts pour s'en assurer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 $K$ plus proches voisins\n",
    "Les images des caractères sont codées par des variables  quantitatives. Le problème de reconnaissance de forme ou de discrimination est adapté à l'algorithme des  [$k$-plus proches voisins](http://wikistat.fr/pdf/st-m-app-add.pdf). Le paramètre à optimiser pour contrôler la complexité du modèle est le nombre de voisin `n_neighbors`. Les options sont décrites dans la [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "digit_knn=knn.fit(X_train, y_train) \n",
    "# Estimation de l'erreur de prévision\n",
    "# sur l'échantillon test\n",
    "1-digit_knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation du paramètre de complexité du modèle par validation croisée en cherchant l'erreur minimale sur une grille de valeurs du paramètre avec `cv=5`-*fold cross validation* et `n_jobs=1` pour une exécution en parallèle utilisant tous les processeurs sauf 1. Attention, comme la validation croisée est aléatoire, deux exécutions successives ne donnent pas le même résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "# grille de valeurs\n",
    "param=[{\"n_neighbors\":list(range(1,15))}]\n",
    "knn= GridSearchCV(KNeighborsClassifier(),param,cv=5,n_jobs=-1)\n",
    "digit_knn=knn.fit(X_train, y_train)\n",
    "# paramètre optimal\n",
    "digit_knn.best_params_[\"n_neighbors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Le modèle est estimé avec la valeur \"optimale\" du paramètre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=digit_knn.best_params_[\"n_neighbors\"])\n",
    "digit_knn=knn.fit(X_train, y_train) \n",
    "# Estimation de l'erreur de prévision\n",
    "1-digit_knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prévision\n",
    "y_chap = digit_knn.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_test,y_chap)\n",
    "print(table)\n",
    "plt.matshow(table)\n",
    "plt.title(\"Matrice de Confusion\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Régression logistique\n",
    "La prévision de la survie, variable binaire des données \"Titanic\", se prêtent à une [régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf). Les versions pénalisées (Ridge, Lasso, elastic net, lars) du modèle linéaire général sont les algorithmes les plus développés dans `Scikit-learn` au détriment de ceux plus classique de sélection de variables.  Une version Lasso de la régression logistique est testée afin d'introduire la sélection automatique des variables.\n",
    "\n",
    "Estimation et erreur de prévision du modèle complet sur l'échantillon test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression()\n",
    "titan_logit=logit.fit(T_train, z_train)\n",
    "# Erreur\n",
    "1-titan_logit.score(T_test, z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "titan_logit.coef_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour le modèle linéaire, il faudrait construire les commandes d'aide à l'interprétation des résultats.\n",
    "\n",
    "Pénalisation et optimisation du paramètre par validation croisée. Il existe une fonction spécifique mais son mode d'emploi est peu documenté; `GridSearchCV` lui est préférée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grille de valeurs\n",
    "param=[{\"C\":[0.01,0.096,0.098,0.1,0.12,1,10]}]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\"),\n",
    "   param,cv=5,n_jobs=-1)\n",
    "titan_logit=logit.fit(T_train, z_train)\n",
    "# paramètre optimal\n",
    "titan_logit.best_params_[\"C\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Estimation du modèle \"optimal\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=0.98,penalty=\"l1\")\n",
    "titan_logit=logit.fit(T_train, z_train)\n",
    "# Erreur\n",
    "1-titan_logit.score(T_test, z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "titan_logit.coef_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Arbre de décision\n",
    "#### Implémentation\n",
    "Les [arbres binaires de décision](http://wikistat.fr/pdf/st-m-app-cart.pdf) (CART: *classification and regression trees*) s'appliquent à tous types de variables. Les options de l'algorithme sont décrites dans la [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). La complexité du modèle est gérée par deux paramètres : `max_depth`, qui détermine le nombre max de feuilles dans l'arbre, et le nombre minimales `min_samples_split` d'observations requises pour rechercher une dichotomie. \n",
    "\n",
    "**Attention**: Même s'il s'agit d'une implémentation proche de celle originale proposée par Breiman et al. (1984) il n'existe pas comme dans R (package `rpart`) un paramètre de pénalisation de la déviance du modèle par sa complexité (nombre de feuilles) afin de construire une séquence d'arbres emboîtés dans la perspective d'un élagage (*pruning*) optimal par validation croisée. La fonction générique de $k$-*fold cross validation* `GridSearchCV` est utilisée pour optimiser le paramètre de profondeur mais avec beaucoup de précision dans l'élagage. Car ce dernier élimine tout un niveau et pas les seules feuilles inutiles à la qualité de la prévision.\n",
    "\n",
    "En revanche, l'implémentation anticipe sur celles des méthodes d'agrégation de modèles en intégrant les paramètres (nombre de variables tirées, importance...) qui leurs sont spécifiques. D'autre part, la représentation graphique d'un arbre n'est pas incluse et nécessite l'implémentation d'un autre logiciel libre: [Graphviz](http://www.graphviz.org/). \n",
    "\n",
    "Tout ceci souligne encore les objectifs de développement de cette librairie: temps de calcul et prévision brute au détriment d'une recherche d'interprétation. Dans certains exemples éventuellement pas trop compliqués, un arbre élagué de façon optimal peut en effet prévoir à peine moins bien (différence non significative) qu'une agrégation de modèles (forêt aléatoire ou **boosting**) et apporter un éclairage nettement plus pertinent qu'un algorithme de type \"boîte noire\". \n",
    "#### Titanic\n",
    "Estimation de l'arbre complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree=DecisionTreeClassifier()\n",
    "digit_tree=tree.fit(T_train, z_train) \n",
    "# Estimation de l'erreur de prévision\n",
    "1-digit_tree.score(T_test,z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation du paramètre de complexité du modèle par validation croisée en cherchant l'erreur minimale sur une grille de valeurs du paramètre avec `cv=5`-*fold cross validation* et `n_jobs=-1` pour une exécution en parallèle utilisant tous les processeurs sauf 1. Attention, comme la validation croisée est aléatoire et un arbre un modèle instable, deux exécutions successives ne donnent pas nécessairement le même résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param=[{\"max_depth\":list(range(2,10))}]\n",
    "titan_tree= GridSearchCV(DecisionTreeClassifier(),param,cv=5,n_jobs=-1)\n",
    "titan_opt=titan_tree.fit(T_train, z_train)\n",
    "# paramètre optimal\n",
    "titan_opt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La valeur \"optimale\" du paramètre est encore trop importante. Une valeur plus faible est utilisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree=DecisionTreeClassifier(max_depth=3)\n",
    "titan_tree=tree.fit(T_train, z_train)\n",
    "# Estimation de l'erreur de prévision\n",
    "# sur l'échantillon test\n",
    "1-titan_tree.score(T_test,z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noter l'amélioration de l'erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prévision de l'échantillon test\n",
    "z_chap = titan_tree.predict(T_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(z_test,z_chap)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracer l'arbre avec le logiciel Graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydot\n",
    "dot_data = StringIO() \n",
    "export_graphviz(titan_tree, out_file=dot_data) \n",
    "graph=pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_png(\"titan_tree.png\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'arbre est généré dans un fichier image à visualiser pour se rende compte qu'il est plutôt mal élagué et pas directement interprétable sans les noms en clair des variables et modalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='titan_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Caractères\n",
    "La même démarche est utilisée pour ces données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Arbre complet\n",
    "tree=DecisionTreeClassifier()\n",
    "digit_tree=tree.fit(X_train, y_train) \n",
    "# Estimation de l'erreur de prévision\n",
    "1-digit_tree.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Optimisation de la profondeur par validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param=[{\"max_depth\":list(range(5,15))}]\n",
    "digit_tree= GridSearchCV(DecisionTreeClassifier(),param,cv=5,n_jobs=-1)\n",
    "digit_opt=digit_tree.fit(X_train, y_train)\n",
    "digit_opt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Estimation de l'arbre \"optimal\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree=DecisionTreeClassifier(max_depth=11)\n",
    "digit_tree=tree.fit(X_train, y_train)\n",
    "# Estimation de l'erreur de prévision\n",
    "1-digit_tree.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prévision de l'échantillon test et matrice de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_chap = digit_tree.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_test,y_chap)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(table)\n",
    "plt.title(\"Matrice de Confusion\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet arbre est bien trop complexe, le résultat est illisible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dot_data = StringIO() \n",
    "export_graphviz(digit_tree, out_file=dot_data) \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_png(\"digit_tree.png\")  \n",
    "Image(filename='digit_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.5 Forêts aléatoires\n",
    "L'algorithme d'agrégation de modèles le plus utilisé est celui des [forêts aléatoires](http://wikistat.fr/pdf/st-m-app-agreg.pdf) (random forest) de Breiman (2001) ce qui ne signifie pas qu'il conduit toujours à la meilleure prévision. Voir la [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) pour la signification de tous les paramètres.\n",
    "\n",
    "Plus que le nombre d'arbres `n_estimators`, le paramètre à optimiser est le nombre de variables tirées aléatoirement pour la recherche de la division optimale d'un noeud: `max_features`. Par défaut, il prend la valeur $p/3$ en  régression et $\\sqrt{p}$ en discrimination.\n",
    "#### Caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# définition des paramètres\n",
    "forest = RandomForestClassifier(n_estimators=500, \n",
    "   criterion='gini', max_depth=None,\n",
    "   min_samples_split=2, min_samples_leaf=1, \n",
    "   max_features='auto', max_leaf_nodes=None,\n",
    "   bootstrap=True, oob_score=True)\n",
    "# apprentissage\n",
    "forest = forest.fit(X_train,y_train)\n",
    "print(1-forest.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-forest.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'optimisation du paramètre `max_features` peut être réalisée en minimisant l'erreur de prévision *out-of-bag*. Ce n'est pas prévu, il est aussi possible comme précédemment de minimiser l'erreur par validation croisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param=[{\"max_features\":list(range(4,64,4))}]\n",
    "digit_rf= GridSearchCV(RandomForestClassifier(n_estimators=100),param,cv=5,n_jobs=-1)\n",
    "digit_rf=digit_rf.fit(X_train, y_train)\n",
    "# paramètre optimal\n",
    "digit_rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de la valeur \"optimale\". Notez que la méthode n'est pas trop sensible à la valeur de ce paramètre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=500,criterion='gini', max_depth=None,min_samples_split=2, \n",
    "                min_samples_leaf=1, max_features=8,max_leaf_nodes=None,bootstrap=True, oob_score=True)\n",
    "# apprentissage\n",
    "forest = forest.fit(X_train,y_train)\n",
    "print(1-forest.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-forest.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prévision\n",
    "y_chap = forest.predict(X_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(y_test,y_chap)\n",
    "print(table)\n",
    "plt.matshow(table)\n",
    "plt.title(\"Matrice de Confusion\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\subsubsection*{Titanic}\n",
    "Même démarche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# définition des paramètres\n",
    "forest = RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=None, \n",
    "    min_samples_split=2, min_samples_leaf=1, max_features='auto', max_leaf_nodes=None,bootstrap=True, oob_score=True)\n",
    "# apprentissage\n",
    "forest = forest.fit(T_train,z_train)\n",
    "print(1-forest.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-forest.score(T_test,z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Optimisation du paramètre {\\tt max\\_features}  par validation croisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param=[{\"max_features\":list(range(2,15))}]\n",
    "titan_rf= GridSearchCV(RandomForestClassifier(n_estimators=100),param,cv=5,n_jobs=-1)\n",
    "titan_rf=titan_rf.fit(T_train, z_train)\n",
    "# paramètre optimal\n",
    "titan_rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de la valeur \"optimale\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=500,criterion='gini', max_depth=None,min_samples_split=2,\n",
    "         min_samples_leaf=1, max_features=6, max_leaf_nodes=None,bootstrap=True, oob_score=True)\n",
    "# apprentissage\n",
    "forest = forest.fit(T_train,z_train)\n",
    "print(1-forest.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# erreur de prévision sur le test\n",
    "1-forest.score(T_test,z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prévision\n",
    "z_chap = forest.predict(T_test)\n",
    "# matrice de confusion\n",
    "table=pd.crosstab(z_test,z_chap)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifier la valeur du paramètre pour constater sa faible influence sur la qualité plutôt médiocre du résultat. \n",
    "\n",
    "**Attention**, comme déjà signalé, l'échantillon test est de relativement faible taille (autour de 180), il serait opportun d'itérer l'extraction aléatoire d'échantillons tests pour tenter de réduire la variance de cette estimation et avoir une idée de sa distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
